import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import networkx as nx
import io
import zipfile
import unicodedata
from copy import deepcopy
from typing import Any, Dict, List, Optional, Tuple

### ìë™í™” ê´€ë ¨ í•¨ìˆ˜ ì„ ì–¸
def _nfc(s: str) -> str:
    return unicodedata.normalize('NFC', s)

def _fix_zip_name(name: str) -> str:
    """
    zipfileì´ cp437ë¡œ ì˜ëª» ë””ì½”ë”©í•œ íŒŒì¼ëª…ì„ ë³µêµ¬ ì‹œë„.
    1) cp437 bytesë¡œ ë˜ëŒë¦° ë’¤
    2) utf-8 / cp949 ìˆœìœ¼ë¡œ decode ì‹œë„
    """
    try:
        raw = name.encode("cp437")
    except Exception:
        return name

    for enc in ("utf-8", "cp949"):
        try:
            return raw.decode(enc)
        except Exception:
            pass

    # ìµœí›„: cp949ë¡œ ê¹¨ì§€ë”ë¼ë„ replace
    return raw.decode("cp949", errors="replace")


def _pick_excel_from_zip(z: zipfile.ZipFile, original_filename_no_ext: str):
    """ZIP ë‚´ë¶€ì—ì„œ ì›ë³¸ íŒŒì¼ëª… ê¸°ë°˜ ë§¤ì¹­ -> ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì—‘ì…€ fallback"""

    infos = []
    for info in z.infolist():
        raw = info.filename
        fixed = _nfc(_fix_zip_name(raw)).replace("\\", "/")

        # __MACOSX ì œê±° + ì—‘ì…€ë§Œ
        if fixed.startswith("__MACOSX") or "/__MACOSX/" in fixed:
            continue
        if not fixed.endswith((".xlsx", ".xls")):
            continue

        infos.append((info, fixed))

    # (í‘œì‹œìš©) clean name
    clean_names = []
    info_by_clean = {}
    for info, fixed in infos:
        base = fixed.split("/")[-1]
        clean_no_ext = base.rsplit(".", 1)[0]
        clean_names.append(clean_no_ext)
        info_by_clean[clean_no_ext] = info   # âœ… ZipInfo ì €ì¥

    # 1) ìë™ ë§¤ì¹­
    norm_orig = _nfc(original_filename_no_ext)
    for clean in clean_names:
        parts = [x for x in clean.split("_") if x]
        parts = [_nfc(x) for x in parts]
        if parts and all(part in norm_orig for part in parts):
            return clean, info_by_clean[clean], "matched"

    # 2) fallback: ì²« ë²ˆì§¸ ì—‘ì…€
    if clean_names:
        clean = clean_names[0]
        return clean, info_by_clean[clean], "fallback_first"

    return None, None, "no_excel"



def prepare_batch_preview(alpha_file, original_filename_no_ext: str):
    """
    1) ZIPì´ë©´ ë§¤ì¹­ í›„ batch_df ë¡œë“œ / ì—‘ì…€ì´ë©´ ë°”ë¡œ ë¡œë“œ
    2) í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° ë¼ì¸ ìƒì„±
    return: (batch_df, meta, preview_lines, summary_lines)
    """
    meta = {
        "uploaded": alpha_file.name,
        "kind": "zip" if alpha_file.name.endswith(".zip") else "excel",
        "matched_file": None,
        "match_mode": None
    }

    # --- 1ë‹¨ê³„: íŒŒì¼ í™•ë³´ (ì—…ë¡œë“œ ì¦‰ì‹œ ì‹¤í–‰) ---
    if alpha_file.name.endswith(".zip"):
        zip_bytes = io.BytesIO(alpha_file.getvalue())
        with zipfile.ZipFile(zip_bytes, 'r') as z:
            matched_clean, matched_info, mode = _pick_excel_from_zip(z, original_filename_no_ext)
            if mode == "no_excel":
                raise ValueError("ZIP ë‚´ë¶€ì— ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            meta["matched_file"] = matched_clean
            meta["match_mode"] = mode

            # âœ… ë¬¸ìì—´ ê²½ë¡œê°€ ì•„ë‹ˆë¼ ZipInfoë¡œ open
            with z.open(matched_info) as f:
                batch_df = pd.read_excel(
                f,
                dtype=str  # <â”€ ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë°›ìŒ (ìˆ«ìë¡œ ì˜¤ì¸ ê¸ˆì§€)
            )

    else:
        meta["matched_file"] = alpha_file.name
        meta["match_mode"] = "no_match_needed"
        batch_df = pd.read_excel(
            alpha_file,
            dtype=str  # <â”€ ì—¬ê¸°ì„œë„ ë™ì¼
        )

    # --- ê²€ì¦/ì •ë¦¬ ---
    needed_cols = {"from", "to", "to_name", "alpha"}
    if not needed_cols.issubset(batch_df.columns):
        raise ValueError(f"ì—‘ì…€ íŒŒì¼ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: {needed_cols}")


    df = batch_df.copy()
    df["from"] = df["from"].astype(str)
    df["to"] = df["to"].astype(str)
    df["to_name"] = df["to_name"].astype(str)
    df["to_name"] = df["to_name"].replace("nan", "").fillna("")
    df["alpha"] = pd.to_numeric(df["alpha"], errors="coerce")

    # alphaê°€ NaNì¸ í–‰ ì œê±°
    df = df.dropna(subset=["alpha"])

    # --- 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° ìƒì„± ---
    preview_lines = []
    for _, r in df.iterrows():
        nm = r["to_name"] if r["to_name"] else "-"
        preview_lines.append(f"{r['from']} -> {r['to']}({nm}) : {float(r['alpha'])*100:.4f}%")

    # fromë³„ í•©/ì”ì—¬
    summary_lines = []
    grouped = df.groupby("from")["alpha"].sum()
    for origin_code, total_alpha in grouped.items():
        remaining = 1.0 - float(total_alpha)
        summary_lines.append(
            f"[from={origin_code}] ì´ë™í•©={float(total_alpha)*100:.4f}%, ì”ì—¬={remaining*100:.4f}%"
        )

    return df, meta, preview_lines, summary_lines

### ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì„ ì–¸
def make_binary_matrix(matrix, threshold):
    # ì„ê³„ê°’ ì´í•˜ì˜ ì›ì†Œë“¤ì„ 0ìœ¼ë¡œ ì„¤ì •
    binary_matrix = matrix.apply(lambda x: np.where(x > threshold, 1, 0))
    return binary_matrix

def filter_matrix(matrix, threshold):
    # ì„ê³„ê°’ ì´í•˜ì˜ ì›ì†Œë“¤ì„ 0ìœ¼ë¡œ ì„¤ì •
    filtered_matrix = matrix.where(matrix > threshold, 0)
    return filtered_matrix

def calculate_network_centralities(G_bn, df_label, use_weight=False):
    weight_arg = 'weight' if use_weight else None

    # Degree
    in_degree_bn = dict(G_bn.in_degree(weight=weight_arg))
    out_degree_bn = dict(G_bn.out_degree(weight=weight_arg))

    df_degree = df_label.iloc[2:, :2].copy()
    df_degree["in_degree"] = pd.Series(in_degree_bn).sort_index().values.reshape(-1, 1)
    df_degree["out_degree"] = pd.Series(out_degree_bn).sort_index().values.reshape(-1, 1)

    gd_in_mean = df_degree["in_degree"].mean()
    gd_in_std = df_degree["in_degree"].std()
    gd_out_mean = df_degree["out_degree"].mean()
    gd_out_std = df_degree["out_degree"].std()

    # Betweenness
    bc_bn = nx.betweenness_centrality(G_bn, normalized=False, endpoints=False, weight=weight_arg)
    num_n = len(G_bn)
    bc_bn = {node: value / (num_n * (num_n - 1)) for node, value in bc_bn.items()}

    df_bc = df_label.iloc[2:, :2].copy()
    df_bc["Betweenness Centrality"] = pd.Series(bc_bn).sort_index().values.reshape(-1, 1)

    bc_mean = df_bc["Betweenness Centrality"].mean()
    bc_std = df_bc["Betweenness Centrality"].std()

    # Closeness
    cci_bn = nx.closeness_centrality(G_bn, distance=weight_arg)
    cco_bn = nx.closeness_centrality(G_bn.reverse(), distance=weight_arg)

    df_cc = df_label.iloc[2:, :2].copy()
    df_cc["Indegree_Closeness Centrality"] = pd.Series(cci_bn).sort_index().values.reshape(-1, 1)
    df_cc["Outdegree_Closeness Centrality"] = pd.Series(cco_bn).sort_index().values.reshape(-1, 1)

    cc_in_mean = df_cc["Indegree_Closeness Centrality"].mean()
    cc_in_std = df_cc["Indegree_Closeness Centrality"].std()
    cc_out_mean = df_cc["Outdegree_Closeness Centrality"].mean()
    cc_out_std = df_cc["Outdegree_Closeness Centrality"].std()

    # Eigenvector
    evi_bn = nx.eigenvector_centrality(G_bn, max_iter=500, tol=1e-06, weight=weight_arg)
    evo_bn = nx.eigenvector_centrality(G_bn.reverse(), max_iter=500, tol=1e-06, weight=weight_arg)

    df_ev = df_label.iloc[2:, :2].copy()
    df_ev["Indegree_Eigenvector Centrality"] = pd.Series(evi_bn).sort_index().values.reshape(-1, 1)
    df_ev["Outdegree_Eigenvector Centrality"] = pd.Series(evo_bn).sort_index().values.reshape(-1, 1)

    ev_in_mean = df_ev["Indegree_Eigenvector Centrality"].mean()
    ev_in_std = df_ev["Indegree_Eigenvector Centrality"].std()
    ev_out_mean = df_ev["Outdegree_Eigenvector Centrality"].mean()
    ev_out_std = df_ev["Outdegree_Eigenvector Centrality"].std()

    # HITS (ê°€ì¤‘ì¹˜ ì§€ì› ì•ˆ í•¨ â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    hubs, authorities = nx.hits(G_bn, max_iter=1000, tol=1e-08, normalized=True)

    df_hi = df_label.iloc[2:, :2].copy()
    df_hi["HITS Hubs"] = pd.Series(hubs).sort_index().values.reshape(-1, 1)
    df_hi["HITS Authorities"] = pd.Series(authorities).sort_index().values.reshape(-1, 1)

    hi_hub_mean = df_hi["HITS Hubs"].mean()
    hi_hub_std = df_hi["HITS Hubs"].std()
    hi_ah_mean = df_hi["HITS Authorities"].mean()
    hi_ah_std = df_hi["HITS Authorities"].std()

    # Structural Hole Metrics (Constraint & Efficiency)
    constraints, efficiencies = calculate_kim_metrics(G_bn, weight=weight_arg)
    df_kim = df_label.iloc[2:, :2].copy()
    df_kim["Constraint"] = pd.Series(constraints).sort_index().values.reshape(-1, 1)
    df_kim["Efficiency"] = pd.Series(efficiencies).sort_index().values.reshape(-1, 1)

    # í‰ê· (Mean) ë° í‘œì¤€í¸ì°¨(Std) ê³„ì‚°
    kim_const_mean = df_kim["Constraint"].mean()
    kim_const_std = df_kim["Constraint"].std()
    kim_eff_mean = df_kim["Efficiency"].mean()
    kim_eff_std = df_kim["Efficiency"].std()

    return (
        df_degree, df_bc, df_cc, df_ev, df_hi, df_kim,  # df_kim ì¶”ê°€
        gd_in_mean, gd_in_std, gd_out_mean, gd_out_std,
        bc_mean, bc_std,
        cc_in_mean, cc_in_std, cc_out_mean, cc_out_std,
        ev_in_mean, ev_in_std, ev_out_mean, ev_out_std,
        hi_hub_mean, hi_hub_std, hi_ah_mean, hi_ah_std,
        kim_const_mean, kim_const_std, kim_eff_mean, kim_eff_std  # í†µê³„ì¹˜ 4ê°œ ì¶”ê°€
    )

@st.cache_data()
def get_submatrix_withlabel(df, start_row, start_col, end_row, end_col, first_index_of_df, numberoflabel = 2):
    row_indexs = list(range(first_index_of_df[0]-numberoflabel, first_index_of_df[0])) + list(range(start_row, end_row+1))
    col_indexs = list(range(first_index_of_df[1]-numberoflabel, first_index_of_df[1])) + list(range(start_col, end_col+1))
    # print(row_indexs)
    # print(col_indexs)

    submatrix_withlabel = df.iloc[row_indexs, col_indexs]
    return submatrix_withlabel

def reduce_negative_values(df, first_idx, mid_ID_idx):
    # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
    df_editing = df.copy()

    # first_idxì—ì„œ mid_ID_idxê¹Œì§€ì˜ ë²”ìœ„ ìŠ¬ë¼ì´ì‹±
    df_test = df_editing.iloc[first_idx[0]:mid_ID_idx[0], first_idx[1]:mid_ID_idx[1]].apply(pd.to_numeric, errors='coerce')

    # ìŒìˆ˜ ê°’ì´ ìˆëŠ” ìœ„ì¹˜ ì¶”ì  ë° ì¤„ì¸ ê°’ ê³„ì‚°
    reduced_values_per_column = {}

    def reduce_and_track(value, col_index):
        if value < 0:
            # ì¤„ì¼ ê°’ ì €ì¥ (ìŒìˆ˜ ê°’ì˜ ì ˆë°˜)
            reduced_value = value / 2
            if col_index not in reduced_values_per_column:
                reduced_values_per_column[col_index] = 0
            reduced_values_per_column[col_index] += value - reduced_value  # ì›ë˜ ê°’ - ì ˆë°˜ìœ¼ë¡œ ì¤„ì¸ ê°’
            return reduced_value
        return value

    # ìŒìˆ˜ì¸ ê°’ë§Œ 1/2ë¡œ ì¤„ì´ë©´ì„œ ì¶”ì 
    for col_idx in range(df_test.shape[1]):
        df_test.iloc[:, col_idx] = df_test.iloc[:, col_idx].apply(lambda x: reduce_and_track(x, col_idx))

    # ìˆ˜ì •ëœ ê°’ì„ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë‹¤ì‹œ ë°˜ì˜ (first_idxì—ì„œ mid_ID_idxê¹Œì§€ì˜ ë¶€ë¶„)
    df_editing.iloc[first_idx[0]:mid_ID_idx[0], first_idx[1]:mid_ID_idx[1]] = df_test

    # ë§ˆì§€ë§‰ í–‰ì— ì¤„ì¸ ê°’ë§Œí¼ ë”í•˜ê¸°
    last_row_index = df_editing.shape[0] - 1
    for col_idx, total_reduced in reduced_values_per_column.items():
        df_editing.iloc[last_row_index, first_idx[1] + col_idx] -= total_reduced

    msg = "ìŒìˆ˜ ê°’ë“¤ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê³ , ì¤„ì¸ ê°’ì„ ë§ˆì§€ë§‰ í–‰ì— ë”í–ˆìŠµë‹ˆë‹¤."

    # ì¤‘ê°„ ì¸ë±ìŠ¤ ê°’ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜ (mid_ID_idxëŠ” í–‰ê³¼ ì—´ ì¸ë±ìŠ¤ì´ë¯€ë¡œ ì´ ê²½ìš° ë³€ê²½ë˜ì§€ ì•ŠìŒ)
    return df_editing, msg, mid_ID_idx




def get_mid_ID_idx(df, first_idx):
    matrix_X = df.iloc[first_idx[0]:, first_idx[1]:].astype(float)
    row_cnt, col_cnt, row_sum, col_sum = 0, 0, 0, 0
    for v in matrix_X.iloc[0]:
        if abs(row_sum - v) < 0.001:
            if v == 0:
                continue
            else: break
        row_cnt += 1
        row_sum += v
    for v in matrix_X.iloc[:, 0]:
        print(f'gap: {col_sum-v}, sum: {col_sum}, value: {v}')
        if abs(col_sum - v) < 0.001:
            if v == 0:
                continue
            else: break
        col_cnt += 1
        col_sum += v
    
    if row_cnt == col_cnt:
        size = row_cnt
    else:
        size = max(row_cnt, col_cnt)

    return (first_idx[0]+size, first_idx[1]+size)

def insert_row_and_col(df, first_idx, mid_ID_idx, code, name, num_of_label):
    df_editing = df.copy()
    df_editing.insert(loc=mid_ID_idx[1], column='a', value=np.nan, allow_duplicates=True)
    df_editing.iloc[first_idx[0]-num_of_label, mid_ID_idx[1]] = code
    df_editing.iloc[first_idx[0]-num_of_label+1, mid_ID_idx[1]] = name
    df_editing.iloc[first_idx[0]:, mid_ID_idx[1]] = 0
    df_editing.columns = range(df_editing.shape[1])
    df_editing = df_editing.T   
    df_editing.insert(loc=mid_ID_idx[0], column='a', value=np.nan, allow_duplicates=True)
    df_editing.iloc[first_idx[1]-num_of_label, mid_ID_idx[0]] = code
    df_editing.iloc[first_idx[1]-num_of_label+1, mid_ID_idx[0]] = name
    df_editing.iloc[first_idx[1]:, mid_ID_idx[0]] = 0
    df_editing.columns = range(df_editing.shape[1])
    df_editing = df_editing.T
    df_inserted = df_editing.copy()
    mid_ID_idx = (mid_ID_idx[0]+1, mid_ID_idx[1]+1)
    msg = f'A new row and column (Code: {code}, Name: {name}) have been inserted.'

    return df_inserted, mid_ID_idx, msg

def transfer_to_new_sector(df, first_idx, origin_code, target_code, ratio, code_label = 2):
    df_editing = df.copy()
    target_idx = df_editing.index[df_editing[first_idx[1]-code_label] == target_code].tolist()
    if len(target_idx) == 1:
        target_idx = target_idx[0]
    else:
        msg = 'ERROR: target code is not unique.'
        return df_editing, msg
    origin_idx = df_editing.index[df_editing[first_idx[1]-code_label] == origin_code].tolist()
    if len(origin_idx) == 1:
        origin_idx = origin_idx[0]
    else:
        msg = 'ERROR: origin code is not unique.'
        return df_editing, msg
    df_editing.iloc[first_idx[0]:, first_idx[1]:] = df_editing.iloc[first_idx[0]:, first_idx[1]:].apply(pd.to_numeric, errors='coerce')
    origin_idx = (origin_idx, origin_idx-first_idx[0]+first_idx[1])
    target_idx = (target_idx, target_idx-first_idx[0]+first_idx[1])
    df_editing.iloc[target_idx[0] ,first_idx[1]:] += df_editing.iloc[origin_idx[0] ,first_idx[1]:] * ratio
    df_editing.iloc[origin_idx[0] ,first_idx[1]:] = df_editing.iloc[origin_idx[0] ,first_idx[1]:] * (1-ratio)
    df_editing.iloc[first_idx[0]: ,target_idx[1]] += df_editing.iloc[first_idx[0]: ,origin_idx[1]] * ratio
    df_editing.iloc[first_idx[0]: ,origin_idx[1]] = df_editing.iloc[first_idx[0]: ,origin_idx[1]] * (1-ratio)

    msg = f'{ratio*100}% of {origin_code} has been moved to {target_code}.'
    return df_editing, msg

def remove_zero_series(
    df: pd.DataFrame,
    first_idx: tuple[int, int],
    mid_ID_idx: tuple[int, int],
    remove_positions: dict | None = None,
):
    """
    - remove_positionsê°€ Noneì´ë©´: ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ '0ìœ¼ë¡œë§Œ ì´ë¤„ì§„ í–‰'ì„ ì°¾ì•„ (ëŒ€ì‘ë˜ëŠ” ì—´)ê¹Œì§€ ì‚­ì œ
    - remove_positionsê°€ ì£¼ì–´ì§€ë©´: í•´ë‹¹ ìœ„ì¹˜ë§Œ ì‚­ì œ

    remove_positions / return_positions í˜•ì‹(ë™ì¼):
      {
        "zero_row_indices": [ ... ],   # dfì˜ ì›ë³¸ index ê¸°ì¤€ (dropì— ë°”ë¡œ ì“°ëŠ” ê°’)
        "zero_col_indices": [ ... ],   # dfì˜ ì›ë³¸ column index ê¸°ì¤€ (dropì— ë°”ë¡œ ì“°ëŠ” ê°’)
      }

    return:
      df_editing, msg, mid_ID_idx, removed_positions
    """

    df_editing = df.copy()

    # -------------------------
    # 1) ì‚­ì œ ìœ„ì¹˜ ê²°ì •
    # -------------------------
    if remove_positions is None:
        # ê¸°ì¡´ ë¡œì§: first_idx ì´í›„ ë¸”ë¡ì„ ìˆ«ìë¡œ ë³´ê³ , í–‰ ì „ì²´ê°€ 0ì¸ í–‰ ì°¾ê¸°
        df_test = df_editing.iloc[first_idx[0]:, first_idx[1]:].apply(pd.to_numeric, errors="coerce")

        zero_row_indices = df_test.index[(df_test == 0).all(axis=1)].tolist()
        zero_row_indices = [i for i in zero_row_indices if first_idx[0] <= i <= mid_ID_idx[0]]

        # ê¸°ì¡´ ë¡œì§: row index -> ëŒ€ì‘ë˜ëŠ” col index ë§¤í•‘
        zero_col_indices = [i - first_idx[0] + first_idx[1] for i in zero_row_indices]

        removed_positions = {
            "zero_row_indices": zero_row_indices,
            "zero_col_indices": zero_col_indices,
        }

    else:
        # ì£¼ì–´ì§„ ì‚­ì œ ìœ„ì¹˜ ì‚¬ìš© (í˜•ì‹ ë™ì¼)
        removed_positions = {
            "zero_row_indices": list(remove_positions.get("zero_row_indices", [])),
            "zero_col_indices": list(remove_positions.get("zero_col_indices", [])),
        }

    zero_row_indices = removed_positions["zero_row_indices"]
    zero_col_indices = removed_positions["zero_col_indices"]

    # -------------------------
    # 2) ì‚­ì œ ìˆ˜í–‰
    # -------------------------
    if len(zero_row_indices) > 0:
        df_editing.drop(zero_row_indices, inplace=True)

    if len(zero_col_indices) > 0:
        df_editing.drop(zero_col_indices, inplace=True, axis=1)

    # index/columns ë¦¬ì…‹(ë„ˆ ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    df_editing.columns = range(df_editing.shape[1])
    df_editing.index = range(df_editing.shape[0])

    # -------------------------
    # 3) mid_ID_idx ì—…ë°ì´íŠ¸ + msg
    # -------------------------
    count = len(zero_col_indices)  # ê¸°ì¡´ ë¡œì§: ì‚­ì œí•œ ì—´ ê°œìˆ˜ë§Œí¼ mid_ID_idx ì¤„ì„
    msg = f"{count}ê°œì˜ í–‰(ì—´)ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
    mid_ID_idx = (mid_ID_idx[0] - count, mid_ID_idx[1] - count)

    return df_editing, msg, mid_ID_idx, removed_positions

def donwload_data(df, file_name):
    csv = convert_df(df)
    button = st.download_button(label=f"{file_name} ë‹¤ìš´ë¡œë“œ", data=csv, file_name=file_name+".csv", mime='text/csv')
    return button




@st.cache_data()
def load_data(file, sheet):
    df = pd.read_excel(file, sheet_name=sheet, header=None)
    return df



@st.cache_data 
def convert_df(df):
    return df.to_csv(header=False, index=False).encode('utf-8-sig')


@st.cache_data
def make_zip_bytes(dfs: dict[str, pd.DataFrame]) -> bytes:
    """
    dfs: dict where keys are desired CSV filenames and values are DataFrames.
    ë°˜í™˜ê°’: ZIP íŒŒì¼ì˜ ë°”ì´ë„ˆë¦¬
    """
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for fname, df in dfs.items():
            csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
            zf.writestr(f"{fname}.csv", csv_bytes)
    return buf.getvalue()

def download_multiple_csvs_as_zip(dfs: dict[str, pd.DataFrame], zip_name: str):
    zip_bytes = make_zip_bytes(dfs)
    return st.download_button(
        label=f"{zip_name} ë‹¤ìš´ë¡œë“œ",
        data=zip_bytes,
        file_name=f"{zip_name}.zip",
        mime="application/zip",
    )

def compute_leontief_inverse(A, epsilon=0.05, max_iter=100):
    """
    Leontief ì—­í–‰ë ¬ì„ ë¬´í•œê¸‰ìˆ˜(I + A + A^2 + ...)ë¡œ ê·¼ì‚¬ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜.
    ìˆ˜ë ´ ì¡°ê±´: ëˆ„ì í•©ì˜ ìƒëŒ€ë³€í™”ê°€ epsilon ì´í•˜ê°€ ë  ë•Œê¹Œì§€ ë°˜ë³µ.
    
    Parameters:
        A (ndarray): íˆ¬ì…ê³„ìˆ˜í–‰ë ¬.
        epsilon (float): ìˆ˜ë ´ íŒì • ê¸°ì¤€ (ì˜ˆ: 0.05 = 5%).
        max_iter (int): ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ë¬´í•œê¸‰ìˆ˜ì˜ ìˆ˜ë ´ì´ ì•ˆ ë  ê²½ìš° ëŒ€ë¹„).
    
    Returns:
        M (ndarray): I + A + A^2 + ... + A^r (rë²ˆì§¸ í•­ê¹Œì§€ ê³„ì‚°í•œ ê·¼ì‚¬ Leontief ì—­í–‰ë ¬).
    """
    n = A.shape[0]
    I = np.eye(n)           # n x n í•­ë“±í–‰ë ¬ ìƒì„±
    M = I.copy()            # ì´ˆê¸° ëˆ„ì í•©: M(0) = I
    s_prev = np.sum(M)      # ì´ˆê¸° ì „ì²´í•© (s(0))
    k = 1                   # ê±°ë“­ì œê³± ì°¨ìˆ˜ ì´ˆê¸°í™”

    while k < max_iter:
        # A^k ê³„ì‚° (í–‰ë ¬ì˜ ê±°ë“­ì œê³±)
        A_power = np.linalg.matrix_power(A, k)
        
        # ëˆ„ì í•© ì—…ë°ì´íŠ¸: M(k) = M(k-1) + A^k
        M_new = M + A_power
        
        # ìƒˆë¡œìš´ ì „ì²´í•© ê³„ì‚°
        s_new = np.sum(M_new)
        
        # ìƒëŒ€ ë³€í™”ëŸ‰ ê³„ì‚°: (s(k) - s(k-1)) / s(k-1)
        ratio_change = (s_new - s_prev) / s_prev if s_prev != 0 else 0
        
        # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥ (ë””ë²„ê·¸ìš©)
        print(f"Iteration {k}: ratio_change = {ratio_change:.4f}")
        
        # ìˆ˜ë ´ íŒì •: ìƒëŒ€ ë³€í™”ê°€ epsilon ì´í•˜ì´ë©´ ì¢…ë£Œ
        if ratio_change <= epsilon:
            M = M_new
            break
        
        # ì—…ë°ì´íŠ¸ í›„ ë‹¤ìŒ ë°˜ë³µ ì§„í–‰
        M = M_new
        s_prev = s_new
        k += 1
    
    return M

def separate_diagonals(N0):
    """
    ì…ë ¥ í–‰ë ¬ N0ì—ì„œ ëŒ€ê°ì›ì†Œì™€ ë¹„ëŒ€ê°ì›ì†Œ(ë„¤íŠ¸ì›Œí¬ base)ë¥¼ ë¶„ë¦¬.
    
    Parameters:
        N0 (ndarray): Leontief ì—­í–‰ë ¬ ê·¼ì‚¬ (I + A + A^2 + ...).
    
    Returns:
        Diagon (ndarray): N0ì—ì„œ ëŒ€ê°ì›ì†Œë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ë¥¼ 0ìœ¼ë¡œ ë§Œë“  í–‰ë ¬.
        N (ndarray): N0ì—ì„œ ëŒ€ê°ì›ì†Œë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“  ë„¤íŠ¸ì›Œí¬ í–‰ë ¬.
    """
    # np.diag: ëŒ€ê° ì„±ë¶„ ì¶”ì¶œ, np.diagflat: ëŒ€ê° í–‰ë ¬ë¡œ ì¬êµ¬ì„±
    Diagon = np.diag(np.diag(N0))
    N = N0 - Diagon
    return Diagon, N

def threshold_network(N, delta):
    """
    ë„¤íŠ¸ì›Œí¬ í–‰ë ¬ Nì—ì„œ ì„ê³„ì¹˜ deltaë³´ë‹¤ ì‘ì€ ê°’ë“¤ì„ 0ìœ¼ë¡œ ëŒ€ì²´.
    
    Parameters:
        N (ndarray): ì›ë³¸ ë„¤íŠ¸ì›Œí¬ í–‰ë ¬.
        delta (float): ì„ê³„ì¹˜ ê°’.
    
    Returns:
        N_thresholded (ndarray): thresholding ì ìš©ëœ ë„¤íŠ¸ì›Œí¬ í–‰ë ¬.
    """
    N_thresholded = N.copy()
    N_thresholded[N_thresholded < delta] = 0
    return N_thresholded

def create_binary_network(N):
    """
    ê°€ì¤‘ì¹˜ ë„¤íŠ¸ì›Œí¬ í–‰ë ¬ Në¥¼ ì´ì§„(0-1) ë„¤íŠ¸ì›Œí¬ë¡œ ë³€í™˜ (ì–‘ìˆ˜ë©´ 1, ì•„ë‹ˆë©´ 0).
    
    Parameters:
        N (ndarray): ê°€ì¤‘ì¹˜ ë„¤íŠ¸ì›Œí¬ í–‰ë ¬.
    
    Returns:
        BN (ndarray): ì´ì§„í™”ëœ ë„¤íŠ¸ì›Œí¬ (ë°©í–¥ì„± ìœ ì§€).
    """
    BN = (N > 0).astype(int)
    return BN

def create_undirected_network(BN):
    """
    ë°©í–¥ì„±ì´ ìˆëŠ” ì´ì§„ ë„¤íŠ¸ì›Œí¬ BNë¥¼ ë¬´ë°©í–¥ ë„¤íŠ¸ì›Œí¬ë¡œ ë³€í™˜.
    ë‘ ë…¸ë“œ ê°„ ì–´ëŠ í•œìª½ì´ë¼ë„ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©´, ë¬´ë°©í–¥ ì—°ê²°ë¡œ ì²˜ë¦¬.
    
    Parameters:
        BN (ndarray): ì´ì§„í™”ëœ ë°©í–¥ì„± ë„¤íŠ¸ì›Œí¬.
    
    Returns:
        UN (ndarray): ë¬´ë°©í–¥(ëŒ€ì¹­) ì´ì§„ ë„¤íŠ¸ì›Œí¬.
    """
    UN = ((BN + BN.T) > 0).astype(int)
    return UN

@st.cache_data()
def threshold_count(matrix):
    """
    [Integration Logic]
    1. Method 2 (Derivative): ë³€í™”ìœ¨ ì•ˆì •í™” ì§€ì  ê³„ì‚° (ê¸°ì¡´ ìœ ì§€)
    2. Method 2-1 (Distance): ì›ì  ê±°ë¦¬ ìµœì†Œí™” ì§€ì  ê³„ì‚° (ê¸°ì¡´ ìœ ì§€ - ì‹œì‘ì  ì—­í• )
    3. Connectivity Check: Method 2-1 ì§€ì ì—ì„œ ê³ ë¦½ ë…¸ë“œ ë°œìƒ ì‹œ, ì‚¬ë¼ì§ˆ ë•Œê¹Œì§€ Threshold í•˜í–¥ ì¡°ì • (ì‹ ê·œ ì¶”ê°€)
    """
    # -------------------------------------------------------------------------
    # 0. ë°ì´í„° ì¤€ë¹„
    # -------------------------------------------------------------------------
    if hasattr(matrix, 'to_numpy'):
        mat_data = matrix.to_numpy()
    else:
        mat_data = np.array(matrix)
        
    mat_data = mat_data.copy().astype(float)
    np.fill_diagonal(mat_data, 0) # ëŒ€ê° ì„±ë¶„ ì œì™¸
    
    N = mat_data.shape[0]
    total_elements = N**2 - N
    
    # xì¶• ì„¤ì •
    delta = 0.01
    max_val = np.max(mat_data)
    x_values = np.arange(0, max_val + delta, delta)
    
    # -------------------------------------------------------------------------
    # 1. ì§€í‘œ ê³„ì‚°: y(ìƒì¡´ìœ¨) & w(ë³€í™”ìœ¨)
    # -------------------------------------------------------------------------
    # y: Survival Ratio
    y_list = []
    for x in x_values:
        count = (mat_data >= x).sum()
        ratio = count / total_elements
        y_list.append(ratio)
    y = np.array(y_list)

    # w: Slope Change Rate (Method 2)
    if len(y) > 1:
        z = (y[1:] - y[:-1]) / delta
    else:
        z = np.zeros(len(y))

    w_list = []
    w_x_values = []
    for i in range(1, len(z)):
        val_w = abs(z[i] - z[i-1]) / delta 
        w_list.append(val_w)
        if i+1 < len(x_values):
            w_x_values.append(x_values[i+1])
    w = np.array(w_list)
    w_x_values = np.array(w_x_values)
    
    # Method 2: Stability Check (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    epsilon = 0.01
    opt_idx_method2 = 0
    found_method2 = False
    
    for k in range(1, len(w)):
        if k > 3 and (w[k-1] - w[k]) <= epsilon:
            opt_idx_method2 = k + 2
            found_method2 = True
            break
    if not found_method2 and len(x_values) > 0:
        opt_idx_method2 = len(x_values) - 1
    
    threshold_method2 = x_values[opt_idx_method2] if len(x_values) > opt_idx_method2 else 0

    # -------------------------------------------------------------------------
    # 2. Method 2-1 (Distance Minimization) - [ê¸°ì¤€ì ]
    # -------------------------------------------------------------------------
    dist_sq = x_values**2 + y**2
    opt_idx_dist = np.argmin(dist_sq)
    
    threshold_dist = x_values[opt_idx_dist]
    min_y = y[opt_idx_dist] if len(y) > opt_idx_dist else 0

    # -------------------------------------------------------------------------
    # 3. [Logic Addition] Connectivity Backtracking
    # Method 2-1 ì§€ì (opt_idx_dist)ì—ì„œ ì‹œì‘í•˜ì—¬ 0ë°©í–¥ìœ¼ë¡œ ìŠ¤ìº”
    # -------------------------------------------------------------------------
    final_idx = opt_idx_dist
    adjusted = False
    
    # í˜„ì¬ ìµœì ì (Distance Min)ë¶€í„° 0ê¹Œì§€ ì—­ìˆœ íƒìƒ‰
    for idx in range(opt_idx_dist, -1, -1):
        t = x_values[idx]
        
        # Binary Masking
        mask = (mat_data >= t) # 1 if connected, else 0
        
        # ê³ ë¦½ ë…¸ë“œ ì²´í¬ (Undirected ê´€ì : In-degree + Out-degree == 0 ì´ë©´ ê³ ë¦½)
        # mask í–‰ë ¬ì—ì„œ í–‰ì˜ í•©(Out) + ì—´ì˜ í•©(In) ê³„ì‚°
        degrees = mask.sum(axis=1) + mask.sum(axis=0)
        
        if np.any(degrees == 0):
            # ê³ ë¦½ ë…¸ë“œê°€ ì¡´ì¬í•¨ -> Thresholdê°€ ë„ˆë¬´ ë†’ìŒ -> ê³„ì† ë‚®ì¶¤(Loop Continue)
            continue
        else:
            # ê³ ë¦½ ë…¸ë“œ ì—†ìŒ (All Connected) -> ë©ˆì¶¤
            final_idx = idx
            if idx < opt_idx_dist:
                adjusted = True
            break
    
    final_threshold = x_values[final_idx]
    final_y = y[final_idx] if len(y) > final_idx else 0

    # -------------------------------------------------------------------------
    # 4. ì‹œê°í™” (ëª¨ë“  ì§€í‘œ í¬í•¨)
    # -------------------------------------------------------------------------
    fig, ax1 = plt.subplots(figsize=(10, 7))

    # [ì™¼ìª½ ì¶•] y(x) Curve
    color1 = 'tab:blue'
    ax1.set_xlabel('Threshold (x)')
    ax1.set_ylabel('Survival Ratio (y)', color=color1, fontweight='bold')
    ax1.plot(x_values, y, color=color1, label='y: Survival Ratio', linewidth=2, alpha=0.7)
    ax1.tick_params(axis='y', labelcolor=color1)
    ax1.grid(True, alpha=0.3)
    
    # [ì˜¤ë¥¸ìª½ ì¶•] w(t) Curve (ê¸°ì¡´ Method 2 ì‹œê°í™” ìœ ì§€)
    if len(w) > 0:
        ax2 = ax1.twinx()
        color2 = 'tab:orange'
        ax2.set_ylabel('Slope Change Rate (w)', color=color2, fontweight='bold')
        ax2.plot(w_x_values, w, color=color2, linestyle='--', alpha=0.5, label='w: Slope Stability')
        ax2.tick_params(axis='y', labelcolor=color2)

    # [Indicator 1] Method 2 (Stability) - íšŒìƒ‰ ìˆ˜ì§ì„ 
    ax1.axvline(x=threshold_method2, color='gray', linestyle='-.', alpha=0.6,
                label=f'Method 2 (Stable): {threshold_method2:.4f}')

    # [Indicator 2] Method 2-1 (Distance Min) - ë¹¨ê°„ ì  (ì›ë˜ì˜ ìˆ˜í•™ì  ìµœì ì )
    ax1.plot(threshold_dist, min_y, 'ro', markersize=8, alpha=0.6,
             label=f'Method 2-1 (Dist Min): {threshold_dist:.4f}')

    # [Indicator 3] Final Decision (No Isolated) - ì´ˆë¡ìƒ‰ ë³„/X (ìµœì¢… ê²°ì •)
    # ì¡°ì •ì´ ë°œìƒí–ˆë‹¤ë©´ í™”ì‚´í‘œì™€ í•¨ê»˜ í‘œì‹œ
    label_final = f'Final (No Isolated): {final_threshold:.4f}'
    
    if adjusted:
        # ì¡°ì •ëœ ê²½ìš°: Method 2-1 -> Final ë¡œ í™”ì‚´í‘œ í‘œì‹œ
        ax1.annotate('', xy=(final_threshold, final_y), xytext=(threshold_dist, min_y),
                     arrowprops=dict(arrowstyle="->", color='red', lw=2))
        ax1.plot(final_threshold, final_y, 'X', color='red', markersize=12, zorder=10, label=label_final)
    else:
        # ì¡°ì • ì•ˆ ëœ ê²½ìš°: ë¹¨ê°„ ì  ìœ„ì— ì´ˆë¡ìƒ‰ í…Œë‘ë¦¬ ë“±ì„ ì”Œì›Œ ê°•ì¡°
        ax1.plot(final_threshold, final_y, 'g*', markersize=14, zorder=10, label=label_final)

    # ë²”ë¡€ í†µí•©
    lines1, labels1 = ax1.get_legend_handles_labels()
    if len(w) > 0:
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    else:
        ax1.legend(loc='upper right')

    plt.title('Threshold Optimization: Distance Min + Connectivity Check')
    fig.tight_layout()
    st.pyplot(fig)
    
    # -------------------------------------------------------------------------
    # 5. ê²°ê³¼ ë°˜í™˜ ë° ì„¤ëª…
    # -------------------------------------------------------------------------
    msg_adjustment = ""
    if adjusted:
        msg_adjustment = f"âš ï¸ ìˆ˜í•™ì  ìµœì ì (`{threshold_dist:.4f}`)ì—ì„œ ê³ ë¦½ ë…¸ë“œê°€ ë°œê²¬ë˜ì–´, `{final_threshold:.4f}` ë¡œ í•˜í–¥ ì¡°ì •í–ˆìŠµë‹ˆë‹¤."
    else:
        msg_adjustment = f"âœ… ìˆ˜í•™ì  ìµœì ì (`{threshold_dist:.4f}`)ì´ ê³ ë¦½ ë…¸ë“œ ì—†ì´ ì•ˆì •ì ì…ë‹ˆë‹¤."

    st.markdown(f"""
    **ìµœì  ì„ê³„ê°’ ë¶„ì„ ê²°ê³¼**
    - **Stability Criterion:** `{threshold_method2:.4f}`
    - **Distance Min Criterion:** `{threshold_dist:.4f}` (Backtracking ì‹œì‘ì )
    - **Final Decision:** `{final_threshold:.4f}`
    
    {msg_adjustment}
    """)
    
    return final_threshold

@st.cache_data
def threshold_count_2(matrix):
    """
    Method A: ë¬´í•œê¸‰ìˆ˜(Infinite Series) í™•ì¥ì„ í†µí•œ ë„¤íŠ¸ì›Œí¬ ì¶”ì¶œ
    êµ¬ì¡°: threshold_count í•¨ìˆ˜ì™€ ë™ì¼í•œ íë¦„ (ê³„ì‚° -> ì‹œê°í™” -> ê²°ê³¼ë°˜í™˜)
    """
    # -------------------------------------------------------------------------
    # 0. ë°ì´í„° ì¤€ë¹„
    # -------------------------------------------------------------------------
    if hasattr(matrix, 'to_numpy'):
        mat_data = matrix.to_numpy()
    else:
        mat_data = np.array(matrix)

    A = mat_data.copy().astype(float)
    np.fill_diagonal(A, 0) # ëŒ€ê° ì„±ë¶„ 0 ì²˜ë¦¬

    n = A.shape[0]

    # íŒŒë¼ë¯¸í„° ì„¤ì • (Pseudo-code ê¸°ì¤€)
    epsilon = 0.1          # 10% ê¸°ì¤€
    max_iter = 20          # ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© ì•ˆì „ ì¥ì¹˜

    # ì´ˆê¸°ê°’ (k=0)
    N_accum = np.zeros((n, n)) # N0
    s_accum = 0.0              # s0

    # ì‹œê°í™”ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    k_list = []
    ratio_list = []
    s_list = []

    # -------------------------------------------------------------------------
    # 1. Iteration: M(k) = A^k ë° Reduce ìˆ˜í–‰
    # -------------------------------------------------------------------------
    final_k = 0
    converged = False

    # këŠ” 1ë¶€í„° ì‹œì‘
    for k in range(1, max_iter + 1):
        # M(k) = A^k
        try:
            M_k = np.linalg.matrix_power(A, k)
        except:
            break # ìˆ˜ì¹˜ì  ë°œì‚° ë“± ì—ëŸ¬ ì‹œ ì¤‘ë‹¨

        # s(k) ê³„ì‚°: ëŒ€ê° ì„±ë¶„ ì œì™¸ ì›ì†Œ í•©
        off_diag_mask = ~np.eye(n, dtype=bool)
        vals = M_k[off_diag_mask]
        s_k = np.sum(vals)

        # av(k) ê³„ì‚°: í‰ê· 
        if (n*n - n) > 0:
            av_k = s_k / (n*n - n)
        else:
            av_k = 0

        # "M(k) reduce": av(k)ë³´ë‹¤ ì‘ì€ ì›ì†Œ 0 ì²˜ë¦¬ (Local Copy)
        M_k_reduced = np.where(M_k < av_k, 0, M_k)

        # Reduced ëœ ê°’ ê¸°ì¤€ìœ¼ë¡œ s(k) ì¬ê³„ì‚° (ëˆ„ì ì„ ìœ„í•´)
        vals_reduced = M_k_reduced[off_diag_mask]
        s_k_reduced = np.sum(vals_reduced)

        # ratio_change ê³„ì‚°
        # Pseudo-codeì˜ (s0 + s(k))/s0 ë…¼ë¦¬ëŠ” í•­ìƒ > 1 ì´ë¯€ë¡œ,
        # ìˆ˜ë ´ íŒë‹¨ì„ ìœ„í•´ 'ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” ì •ë³´ëŸ‰ì˜ ë¹„ìœ¨' (s_k / s0)ë¡œ í•´ì„í•˜ì—¬ êµ¬í˜„
        if s_accum == 0:
            ratio_change = 1.0 # ì²« í„´ì€ ë¬´ì¡°ê±´ ì§„í–‰
        else:
            ratio_change = s_k_reduced / s_accum

        # ê¸°ë¡ ì €ì¥
        k_list.append(k)
        ratio_list.append(ratio_change)
        s_list.append(s_accum + s_k_reduced)

        # ëˆ„ì  ìˆ˜í–‰: N0 = N0 + M(k), s0 = s0 + s(k)
        N_accum = N_accum + M_k_reduced
        s_accum = s_accum + s_k_reduced
        final_k = k

        # ì¢…ë£Œ ì¡°ê±´ (Convergence Check)
        if k > 1 and ratio_change <= epsilon:
            converged = True
            break

    # -------------------------------------------------------------------------
    # 2. ì‹œê°í™” (Dual Axis: Change Ratio vs Total Info)
    # -------------------------------------------------------------------------
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # [ì™¼ìª½ ì¶•] ë³€í™”ìœ¨ (Convergence Ratio)
    color1 = 'tab:red'
    ax1.set_xlabel('Iteration (k)')
    ax1.set_ylabel('Change Ratio (New/Total)', color=color1, fontweight='bold')
    ax1.plot(k_list, ratio_list, color=color1, marker='o', label='Ratio Change', linewidth=2)
    ax1.tick_params(axis='y', labelcolor=color1)
    ax1.grid(True, alpha=0.3)

    # Epsilon ê¸°ì¤€ì„ 
    ax1.axhline(y=epsilon, color='gray', linestyle='--', label=f'Epsilon ({epsilon})')

    # [ì˜¤ë¥¸ìª½ ì¶•] ëˆ„ì  ì •ë³´ëŸ‰ (Total Sum s0)
    ax2 = ax1.twinx()
    color2 = 'tab:blue'
    ax2.set_ylabel('Accumulated Signal (s0)', color=color2, fontweight='bold')
    ax2.plot(k_list, s_list, color=color2, linestyle='--', alpha=0.6, label='Total Signal (s0)')
    ax2.tick_params(axis='y', labelcolor=color2)

    # ë²”ë¡€ í•©ì¹˜ê¸°
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

    plt.title(f'Method A Convergence: Stopped at k={final_k}')
    fig.tight_layout()
    st.pyplot(fig)

    # -------------------------------------------------------------------------
    # 3. ì‚¬ìš©ì ì„ íƒ UI / ê²°ê³¼ ì•ˆë‚´
    # -------------------------------------------------------------------------
    status_msg = "ìˆ˜ë ´ ì™„ë£Œ (Converged)" if converged else "ìµœëŒ€ ë°˜ë³µ ë„ë‹¬ (Max Iter)"

    st.markdown(f"""
    **Method A ì¶”ì¶œ ê²°ê³¼**
    - **ìµœì¢… ë°˜ë³µ íšŸìˆ˜ (k):** `{final_k}` ({status_msg})
    - **ìµœì¢… ëˆ„ì  ì •ë³´ëŸ‰ (s0):** `{s_accum:.4f}`
    - **ë§ˆì§€ë§‰ ë³€í™”ìœ¨:** `{ratio_list[-1]:.4f}` (ëª©í‘œ: $\le {epsilon}$)
    
    ğŸ’¡ **ì„¤ëª…:** í–‰ë ¬ì˜ ê±°ë“­ì œê³±($A^k$)ì„ í†µí•´ ê°„ì ‘ ì—°ê²°ì„ íƒìƒ‰í•˜ë©°, ì •ë³´ëŸ‰ ì¦ê°€ë¶„ì´ {epsilon*100}% ì´í•˜ê°€ ë  ë•Œê¹Œì§€ ë„¤íŠ¸ì›Œí¬ë¥¼ ëˆ„ì í–ˆìŠµë‹ˆë‹¤.
    """)

    # ì‚¬ìš©ìê°€ ì›í•˜ëŠ” network(í–‰ë ¬) ìì²´ë¥¼ ë°˜í™˜
    return N_accum

def calculate_kim_metrics(G, weight='weight'):
    """
    Kim (2021) ë°©ì‹ì˜ Constraintì™€ Efficiencyë¥¼ ê³„ì‚°í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    Return: (constraints_dict, efficiencies_dict)
    """
    # 1. Constraint (Burt's constraint)
    # ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´ ìƒì‚°ìœ ë°œê³„ìˆ˜ ë“±ì„ ë°˜ì˜
    constraints = nx.constraint(G, weight=weight)
    
    # 2. Efficiency (Kim's redundancy-based)
    efficiencies = {}
    nodes = list(G.nodes())
    
    # íš¨ìœ¨ì„± ê³„ì‚°ì„ ìœ„í•œ ì‚¬ì „ ê³„ì‚° (ì†ë„ ìµœì í™”)
    # ì–‘ë°©í–¥ ê±°ë˜ëŸ‰(volume) ê³„ì‚° í—¬í¼
    def get_vol(u, v):
        if not G.has_edge(u, v): return 0.0
        return G[u][v].get(weight, 1.0) if weight else 1.0

    def get_bi_vol(u, v):
        return get_vol(u, v) + get_vol(v, u)

    node_total_volumes = {} # ë¶„ëª¨: (In + Out sum)
    node_max_volumes = {}   # ë¶„ëª¨: Max connection strength
    
    for n in nodes:
        # Total Volume (In + Out)
        vol_in = G.in_degree(n, weight=weight)
        vol_out = G.out_degree(n, weight=weight)
        node_total_volumes[n] = vol_in + vol_out
        
        # Max Volume with any partner
        partners = set(G.predecessors(n)) | set(G.successors(n))
        max_vol = 0.0
        for p in partners:
            vol = get_bi_vol(n, p)
            if vol > max_vol:
                max_vol = vol
        node_max_volumes[n] = max_vol

    # ê°œë³„ ë…¸ë“œ íš¨ìœ¨ì„± ê³„ì‚°
    for i in nodes:
        partners_i = list(set(G.predecessors(i)) | set(G.successors(i)))
        Ni = len(partners_i)
        
        if Ni == 0:
            efficiencies[i] = 0.0
            continue
            
        sum_Rij = 0.0
        for j in partners_i:
            # jì™€ ië¥¼ ì œì™¸í•œ ì œ3ì(q) íƒìƒ‰ (Redundancy check)
            potential_qs = [q for q in partners_i if q != j and q != i]
            
            R_ij = 0.0
            for q in potential_qs:
                # rho_iq: iì˜ ì „ì²´ ê±°ë˜ ì¤‘ qì™€ì˜ ë¹„ì¤‘
                vol_iq = get_bi_vol(i, q)
                denom_i = node_total_volumes.get(i, 0)
                rho_iq = vol_iq / denom_i if denom_i > 1e-9 else 0.0
                
                # tau_jq: jì˜ ìµœëŒ€ ê±°ë˜ ëŒ€ë¹„ qì™€ì˜ ê°•ë„
                vol_jq = get_bi_vol(j, q)
                max_vol_j = node_max_volumes.get(j, 0)
                tau_jq = vol_jq / max_vol_j if max_vol_j > 1e-9 else 0.0
                
                R_ij += (rho_iq * tau_jq)
            sum_Rij += R_ij
        
        # Kim's Efficiency Formula: epsilon = T_i / N_i where T_i = N_i - sum(R_ij)
        Ti = Ni - sum_Rij
        efficiencies[i] = Ti / Ni if Ni > 0 else 0.0
        
    return constraints, efficiencies

def calculate_standard_metrics(G_directed, weight='weight'):
    """Burt í‘œì¤€ ë°©ì‹ (Efficiency = Effective Size / Out-Degree)"""
    std_constraints = nx.constraint(G_directed, weight=weight)
    effective_sizes = nx.effective_size(G_directed, weight=weight)
    
    std_efficiencies = {}
    for n, eff_size in effective_sizes.items():
        degree = G_directed.out_degree(n) # Standard Burt uses Out-degree for ego network size
        if degree > 0:
            std_efficiencies[n] = eff_size / degree
        else:
            std_efficiencies[n] = 0.0
            
    return std_constraints, std_efficiencies

def build_leontief_outputs(
    df_for_leontief: pd.DataFrame,
    normalization_denominator_replaced,
):
    """
      1) df_for_leontief_with_label  (ë¼ë²¨ í¬í•¨ + ë ˆì˜¨í‹°ì—í”„ ì—­í–‰ë ¬ Lë§Œ, FL/BL ì—†ìŒ)
      2) df_for_leontief_without_label (ë¼ë²¨ ì œê±° + Lë§Œ)
      3) fl_bl (ë²ˆí˜¸/ë¶€ë¬¸ëª…ì¹­ + FL + BL)
    """

    # 1) with/without ì¤€ë¹„ (ë„ˆ ì½”ë“œ ë™ì¼)
    df_with_label = df_for_leontief.copy()
    df_without_label = df_with_label.iloc[2:, 2:].copy()

    # 2) A(tmp) ë§Œë“¤ê¸°: ìˆ«ì ë³€í™˜ + ì—´ ì •ê·œí™” (ë„ˆ ì½”ë“œ ë™ì¼)
    tmp = df_without_label.copy()
    tmp = tmp.apply(pd.to_numeric, errors="coerce")
    tmp = tmp.divide(normalization_denominator_replaced, axis=1)

    # Aë¥¼ with_labelì— ë°˜ì˜ (ë„ˆ ì½”ë“œ ë™ì¼)
    df_with_label.iloc[2:, 2:] = tmp

    # 3) ë ˆì˜¨í‹°ì—í”„ ì—­í–‰ë ¬ L=(I-A)^-1 (ë„ˆ ì½”ë“œ ë™ì¼)
    unit_matrix = np.eye(tmp.shape[0])
    subtracted_matrix = unit_matrix - tmp
    leontief = np.linalg.inv(subtracted_matrix.values)
    leontief = pd.DataFrame(leontief)

    # 4) (N+1)x(N+1)ë¡œ í™•ì¥í•´ì„œ FL/BL ê³„ì‚° + í‰ê·  ì •ê·œí™” (ë„ˆ ì½”ë“œ ë™ì¼)
    leontief_rows, leontief_cols = leontief.shape
    leontief_with_sums = np.zeros((leontief_rows + 1, leontief_cols + 1))
    leontief_with_sums[:-1, :-1] = leontief.values
    leontief_with_sums[-1, :-1] = leontief.sum(axis=0).values  # BL ì›ìë£Œ(ì—´í•©)
    leontief_with_sums[:-1, -1] = leontief.sum(axis=1).values  # FL ì›ìë£Œ(í–‰í•©)

    last_row_mean = leontief_with_sums[-1, :-1].mean()
    leontief_with_sums[-1, :-1] /= last_row_mean

    last_col_mean = leontief_with_sums[:-1, -1].mean()
    leontief_with_sums[:-1, -1] /= last_col_mean

    new_df = pd.DataFrame(leontief_with_sums)

    # 5) current_df í™•ì¥ í›„, (2,2)ë¶€í„° new_df ì‚½ì… (ë„ˆ ì½”ë“œ ë™ì¼)
    current_df = df_with_label
    existing_rows = current_df.shape[0] - 2
    existing_cols = current_df.shape[1] - 2

    current_df = current_df.reindex(
        index=range(existing_rows + 3),
        columns=range(existing_cols + 3)
    )

    current_df.iloc[2:2 + new_df.shape[0], 2:2 + new_df.shape[1]] = new_df.values
    current_df.iloc[1, -1] = "FL"
    current_df.iloc[-1, 1] = "BL"

    # 6) fl_bl ì¶”ì¶œ (ë„ˆ ì½”ë“œì˜ iloc ìœ„ì¹˜ ê·¸ëŒ€ë¡œ)
    ids_col = current_df.iloc[1:-1, :2]
    fl_data = current_df.iloc[1:-1, -1]
    bl_data = current_df.iloc[-1, 1:-1]

    fl_data = fl_data.to_frame(name="2")
    bl_data = bl_data.to_frame(name="3")

    ids_col = ids_col.reset_index(drop=True)
    fl_data = fl_data.reset_index(drop=True)
    bl_data = bl_data.reset_index(drop=True)

    fl_bl = pd.concat([ids_col, fl_data, bl_data], axis=1)

    # 7) ìµœì¢… with_labelì—ì„œëŠ” FL/BL ì œê±° (ë„ˆ ì½”ë“œ ë™ì¼)
    df_for_leontief_with_label = current_df.iloc[:-1, :-1].copy()

    # 8) ìµœì¢… without_label ê°±ì‹  (ë„ˆ ì½”ë“œ ë™ì¼)
    df_for_leontief_without_label = df_for_leontief_with_label.iloc[2:, 2:].copy()

    return df_for_leontief_with_label, df_for_leontief_without_label


def apply_batch_edit(
    *,
    batch_df: pd.DataFrame,
    df_curr: pd.DataFrame,
    first_idx: tuple,
    number_of_label: int,
    mid_ID_idx: tuple,
    ids_simbol: dict,
    insert_row_and_col_fn,
):
    """
    Inputs
    ------
    batch_df : columns = ['from','to','alpha','to_name']
    df_curr  : í˜„ì¬ df_editing
    first_idx, number_of_label : ê¸°ì¡´ ê·¸ëŒ€ë¡œ
    mid_ID_idx : í˜„ì¬ mid index
    ids_simbol : ì½”ë“œ->ì´ë¦„ ë¦¬ìŠ¤íŠ¸ dict (ê³µìœ /ê°±ì‹ )
    insert_row_and_col_fn : ê¸°ì¡´ í•¨ìˆ˜ ì£¼ì…

    Returns
    -------
    df_out, mid_out, ids_out, log_text
    """

    df_curr = df_curr.copy()
    code_col_idx = first_idx[1] - number_of_label

    log_lines = []

    # -------------------------
    # 1) to/to_name ê¸°ë°˜ ìë™ ì‚°ì—… ì¶”ê°€
    # -------------------------
    targets = batch_df[["to", "to_name"]].drop_duplicates()

    for _, t in targets.iterrows():
        new_code = str(t["to"])
        new_name = str(t["to_name"]) if str(t["to_name"]) not in ["nan", "None"] else ""

        exists = (df_curr.iloc[:, code_col_idx].astype(str) == new_code).any()
        if exists:
            if new_code not in ids_simbol:
                ids_simbol[new_code] = []
            if new_name and (new_name not in ids_simbol[new_code]):
                ids_simbol[new_code].append(new_name)
            continue

        result = insert_row_and_col_fn(
            df_curr,
            first_idx,
            mid_ID_idx,
            new_code,
            new_name if new_name else f"NEW_{new_code}",
            number_of_label,
        )

        df_curr, mid_ID_idx = result[0], result[1]
        # ì›ë³¸ ì½”ë“œì˜ data_editing_log += result[2]
        log_lines.append(str(result[2]).strip())

        if new_code not in ids_simbol:
            ids_simbol[new_code] = []
        if new_name:
            ids_simbol[new_code].append(new_name)

    # -------------------------
    # 2) from ê¸°ì¤€ ë¶„ë°° ì´ë™
    # -------------------------
    grouped = batch_df.groupby("from")

    for origin_code, group in grouped:
        origin_indices = df_curr.index[df_curr.iloc[:, code_col_idx] == origin_code].tolist()
        if len(origin_indices) != 1:
            log_lines.append(f"Error: Origin Code '{origin_code}' ìœ ì¼í•˜ì§€ ì•Šê±°ë‚˜ ì—†ìŒ. ìŠ¤í‚µ")
            continue

        origin_row_idx = origin_indices[0]
        origin_col_idx = origin_row_idx - first_idx[0] + first_idx[1]

        # snapshot
        origin_row_data = df_curr.iloc[origin_row_idx, first_idx[1]:].copy()
        origin_col_data = df_curr.iloc[first_idx[0]:, origin_col_idx].copy()

        total_alpha = float(group["alpha"].sum())

        for _, r in group.iterrows():
            target_code = r["to"]
            ratio = float(r["alpha"])

            target_indices = df_curr.index[df_curr.iloc[:, code_col_idx] == target_code].tolist()
            if len(target_indices) != 1:
                log_lines.append(
                    f"Error: Target Code '{target_code}' ìœ ì¼í•˜ì§€ ì•Šê±°ë‚˜ ì—†ìŒ. ({origin_code}->{target_code} ìŠ¤í‚µ)"
                )
                continue

            target_row_idx = target_indices[0]
            target_col_idx = target_row_idx - first_idx[0] + first_idx[1]

            df_curr.iloc[target_row_idx, first_idx[1]:] += origin_row_data * ratio
            df_curr.iloc[first_idx[0]:, target_col_idx] += origin_col_data * ratio

            log_lines.append(f"[Batch] {origin_code} -> {target_code}: {ratio*100:.2f}% ì´ë™")

        remaining_ratio = 1.0 - total_alpha
        if abs(remaining_ratio) < 1e-9:
            remaining_ratio = 0.0

        df_curr.iloc[origin_row_idx, first_idx[1]:] = origin_row_data * remaining_ratio
        df_curr.iloc[first_idx[0]:, origin_col_idx] = origin_col_data * remaining_ratio
        log_lines.append(f"[Batch Info] {origin_code} ì”ì—¬: {remaining_ratio*100:.4f}%")

    log_text = "\n".join([x for x in log_lines if x])

    return df_curr, mid_ID_idx, ids_simbol, log_text

def replay_edit_ops_on_df(
    df_base: pd.DataFrame,
    mid_ID_idx_base: Tuple[int, int],
    ids_simbol_base: Dict[str, List[str]],
    ops: List[Dict[str, Any]],
    *,
    first_idx: Tuple[int, int],
    number_of_label: int,
    insert_row_and_col_fn,
    transfer_to_new_sector_fn,
    remove_zero_series_fn,
    reduce_negative_values_fn,
    batch_apply_fn=None,          # apply_batch_edit ê°™ì€ í•¨ìˆ˜ ì£¼ì…
    copy_ids: bool = False,       # ids_simbol ê³µìœ  ì‹«ìœ¼ë©´ True
    return_log: bool = True,      # ë””ë²„ê¹…/ê¸°ë¡ìš© ë¡œê·¸ ë°˜í™˜
):
    """
    opsë¥¼ ìˆœì„œëŒ€ë¡œ df_baseì— ë‹¤ì‹œ ì ìš©í•˜ì—¬ df/mid/idsë¥¼ ê°±ì‹ í•´ì„œ ë°˜í™˜.
    - st.session_state ì˜ì¡´ ì—†ìŒ
    - batch_applyëŠ” batch_apply_fnì´ ì£¼ì–´ì¡Œì„ ë•Œë§Œ ì‹¤í–‰ ê°€ëŠ¥

    Returns
    -------
    (df, mid, ids) or (df, mid, ids, log_text)  (return_log=Trueì¼ ë•Œ)
    """

    df = df_base.copy()
    mid = mid_ID_idx_base
    ids = deepcopy(ids_simbol_base) if copy_ids else ids_simbol_base

    log_lines: List[str] = []

    for i, op in enumerate(ops, start=1):
        if "type" not in op:
            raise KeyError(f"[op #{i}] missing key: 'type'")

        t = op["type"]

        # -------------------------
        # 1) ì‚°ì—… ì¶”ê°€ (insert_row_and_col)
        # -------------------------
        if t == "insert_sector":
            for k in ("code", "name"):
                if k not in op:
                    raise KeyError(f"[op #{i} insert_sector] missing key: '{k}'")

            result = insert_row_and_col_fn(
                df,
                first_idx,
                mid,
                op["code"],
                op["name"],
                number_of_label,
            )
            df, mid = result[0], result[1]

            # result[2] = ë¡œê·¸ ë¬¸ìì—´ (ë„ˆ ì½”ë“œ ê¸°ì¤€)
            if len(result) >= 3 and result[2]:
                log_lines.append(str(result[2]).strip())

            # ids ë°˜ì˜
            c = str(op["code"])
            n = str(op["name"])
            if c not in ids:
                ids[c] = []
            if n and n not in ids[c]:
                ids[c].append(n)

        # -------------------------
        # 2) ê°’ ì˜®ê¸°ê¸° (transfer_to_new_sector)
        # -------------------------
        elif t == "transfer":
            for k in ("from", "to", "alpha"):
                if k not in op:
                    raise KeyError(f"[op #{i} transfer] missing key: '{k}'")

            result = transfer_to_new_sector_fn(
                df,
                first_idx,
                op["from"],
                op["to"],
                float(op["alpha"]),
            )
            df = result[0]

            # result[1]ì´ ë¡œê·¸ë¼ë©´ ëˆ„ì (ë„ˆ í•¨ìˆ˜ê°€ ê·¸ë ‡ê²Œ ì£¼ë©´)
            if return_log and len(result) >= 2 and result[1]:
                log_lines.append(str(result[1]).strip())

        # -------------------------
        # 3) 0ì¸ í–‰/ì—´ ì‚­ì œ (remove_zero_series)
        # -------------------------
        elif t == "remove_zero":
            result = remove_zero_series_fn(df, first_idx, mid, remove_positions=op.get("remove_positions"))
            df, mid = result[0], result[2]

            if return_log and len(result) >= 2 and result[1]:
                log_lines.append(str(result[1]).strip())

        # -------------------------
        # 4) ìŒìˆ˜ ì ˆë°˜ (reduce_negative_values)
        # -------------------------
        elif t == "reduce_negative":
            # ë„¤ ê¸°ì¡´ ì½”ë“œì²˜ëŸ¼ midë¥¼ -1 í•´ì„œ ë„˜ê¸¸ì§€ ì—¬ë¶€
            use_minus_one = bool(op.get("use_minus_one_mid", True))
            mid_use = (mid[0] - 1, mid[1] - 1) if use_minus_one else mid

            result = reduce_negative_values_fn(df, first_idx, mid_use)
            df = result[0]

            if return_log and len(result) >= 2 and result[1]:
                log_lines.append(str(result[1]).strip())

        # -------------------------
        # 5) ë°°ì¹˜ ì ìš© (apply_batch_editë¡œ ì¬ì‹¤í–‰)
        # -------------------------
        elif t == "batch_apply":
            if "batch_records" not in op:
                raise KeyError(f"[op #{i} batch_apply] missing key: 'batch_records'")
            if batch_apply_fn is None:
                raise ValueError(
                    "[batch_apply] batch_apply_fnì´ í•„ìš”í•©ë‹ˆë‹¤. "
                    "ì˜ˆ: replay_edit_ops_on_df(..., batch_apply_fn=apply_batch_edit)"
                )

            batch_df = pd.DataFrame(op["batch_records"])

            df, mid, ids, batch_log = batch_apply_fn(
                batch_df=batch_df,
                df_curr=df,
                first_idx=first_idx,
                number_of_label=number_of_label,
                mid_ID_idx=mid,
                ids_simbol=ids,
                insert_row_and_col_fn=insert_row_and_col_fn,
            )

            if return_log and batch_log:
                log_lines.append(str(batch_log).strip())

        else:
            raise ValueError(f"[op #{i}] Unknown op type: {t}")

    if return_log:
        log_text = "\n".join([x for x in log_lines if x])
        return df, mid, ids, log_text

    return df, mid, ids


# ì§€í‘œ ì—´ ë§Œë“œëŠ” í•¨ìˆ˜
def make_col(title: str, vec_1d: np.ndarray, colname: str) -> pd.DataFrame:
    vec_1d = np.asarray(vec_1d, dtype=float).reshape(-1)
    return pd.concat(
        [
            pd.DataFrame([title], columns=[colname]),
            pd.Series(vec_1d).to_frame(name=colname)
        ],
        axis=0
    ).reset_index(drop=True)

# ì§€í‘œ í…Œì´ë¸” ë§Œë“œëŠ” í•¨ìˆ˜
def make_table(base_df, cols: list[pd.DataFrame]) -> pd.DataFrame:
    ids_col = base_df.iloc[1:, :2].reset_index(drop=True)
    return pd.concat([ids_col] + cols, axis=1)